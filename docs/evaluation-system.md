# Evaluation System Documentation

## Overview

The evaluation system provides automated end-to-end testing of the RAG (Retrieval-Augmented Generation) pipeline to measure factual accuracy. It uses an LLM judge (OpenAI GPT) to evaluate whether answers generated by the system are factually correct based on the retrieved context.

## Architecture

### Evaluation Flow

```
Test Question → Context Retrieval → LLM Answer Generation → Judge Evaluation → Store Results
```

1. **Load Test Cases**: Questions from `evaluation_test_docs/test_cases.json`
2. **Retrieve Context**: Use RAG system to find relevant chunks from test collection
3. **Generate Answer**: LLM generates answer based on retrieved context
4. **Judge Evaluation**: OpenAI judge evaluates factual correctness
5. **Store Results**: Save evaluation metrics and detailed results to database

### Judge Evaluation Criteria

The judge evaluates answers based on:
- Answer must be supported by retrieved context
- No fabricated or hallucinated information
- Appropriate response to the question
- Missing information is tracked but acceptable if not in context

### Key Components

- **Test Collection**: Special collection (`evaluation_test_collection`) with fixed ID
- **Test Cases**: JSON file with questions and optional ground truth
- **Judge Service**: LangChain-based OpenAI service with structured output
- **Evaluation Repository**: SQLite storage for runs and results

## Setup

### 1. Prerequisites

```bash
# Install dependencies
poetry install

# Ensure you have OpenAI API key
export OPENAI_EVALUATION_API_KEY=your_openai_api_key
```

### 2. Configuration

Add to `.env`:

```bash
# Enable evaluation initialization
INITIALIZE_EVALUATION=false

# OpenAI for judge service
OPENAI_EVALUATION_API_KEY=sk-...
OPENAI_EVALUATION_MODEL=gpt-5-mini
OPENAI_EVALUATION_TIMEOUT=60

# Test collection settings (optional - defaults provided)
EVALUATION_TEST_COLLECTION_ID=eval-test-collection-00000000-0000-0000-0000-000000000001
EVALUATION_TEST_COLLECTION_NAME=evaluation_test_collection
EVALUATION_TEST_DOCS_DIR=evaluation_test_docs
```

### 3. Prepare Test Data

#### Directory Structure

```
evaluation_test_docs/
├── document1.pdf
├── document2.pdf
└── test_cases.json
```

#### Test Cases Format

`evaluation_test_docs/test_cases.json`:

```json
{
  "test_cases": [
    {
      "id": "test_001",
      "question": "What is BrainDrive?",
      "category": "general_overview",
      "ground_truth": "BrainDrive is an open-source, self-hosted AI platform..."
    }
  ],
  "metadata": {
    "created_date": "2025-01-04",
    "total_questions": 30
  }
}
```

**Fields**:
- `id` (required): Unique test case identifier
- `question` (required): Question to ask the system
- `category` (required): Category for grouping/analysis
- `ground_truth` (optional): Expected answer for future use

### 4. Start Application

On first startup with `INITIALIZE_EVALUATION=false`:

```bash
uvicorn app.main:app --reload
```

The system will:
1. Create evaluation test collection
2. Process PDFs from `evaluation_test_docs/`
3. Index documents in vector store and BM25
4. Load test cases from `test_cases.json`

Check logs for:
```
✅ Initialized judge service with model: gpt-5-mini
✅ Initialized evaluation repository
✅ Evaluation test collection initialized successfully
```

## Usage

### Run Evaluation

**Endpoint**: `POST /api/evaluation/run`

**Request**:
```bash
curl -X POST http://localhost:8000/api/evaluation/run
```

**Response**:
```json
{
  "evaluation_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Evaluation completed successfully"
}
```

**What happens**:
1. Loads all test cases from `test_cases.json`
2. For each question:
   - Retrieves top 5 relevant chunks (hybrid search)
   - Generates answer using system LLM
   - Evaluates answer using judge LLM
   - Stores result with judge reasoning
3. Calculates accuracy metrics
4. Returns evaluation run ID

**Duration**: ~30 seconds for 30 questions (depends on LLM latency)

### Get Results

**Endpoint**: `GET /api/evaluation/results/{evaluation_id}`

**Request**:
```bash
curl http://localhost:8000/api/evaluation/results/{evaluation_id}
```

**Response**:
```json
{
  "evaluation_run": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "collection_id": "eval-test-collection-00000000-0000-0000-0000-000000000001",
    "status": "completed",
    "total_questions": 30,
    "correct_count": 25,
    "incorrect_count": 5,
    "accuracy": 83.3,
    "run_date": "2025-01-04T10:30:00Z",
    "duration_seconds": 45.2,
    "config_snapshot": {
      "llm_model": "llama3.2:8b",
      "embedding_model": "mxbai-embed-large",
      "judge_model": "gpt-5-mini"
    }
  },
  "results": [
    {
      "id": "result_001",
      "test_case_id": "test_001",
      "question": "What is BrainDrive?",
      "ground_truth": "...",
      "retrieved_context": "[Chunk 1]\n...\n[Chunk 2]\n...",
      "llm_answer": "BrainDrive is an open-source...",
      "judge_correct": true,
      "judge_reasoning": "The answer accurately describes BrainDrive based on the retrieved context...",
      "judge_factual_errors": [],
      "judge_missing_info": [],
      "created_at": "2025-01-04T10:30:15Z"
    }
  ]
}
```

### List Evaluation Runs

**Endpoint**: `GET /api/evaluation/runs?limit=50`

**Request**:
```bash
curl http://localhost:8000/api/evaluation/runs
```

**Response**:
```json
{
  "runs": [
    {
      "id": "550e8400-e29b-41d4-a716-446655440000",
      "collection_id": "eval-test-collection-...",
      "status": "completed",
      "total_questions": 30,
      "correct_count": 25,
      "incorrect_count": 5,
      "accuracy": 83.3,
      "run_date": "2025-01-04T10:30:00Z",
      "duration_seconds": 45.2
    }
  ],
  "total": 1
}
```

## Database Schema

### evaluation_runs

| Column | Type | Description |
|--------|------|-------------|
| id | String | Primary key |
| collection_id | String | Test collection ID |
| status | String | pending/running/completed/failed |
| total_questions | Integer | Number of test cases |
| correct_count | Integer | Number of correct answers |
| incorrect_count | Integer | Number of incorrect answers |
| run_date | DateTime | When evaluation started |
| duration_seconds | Float | Total execution time |
| config_snapshot | JSON | System configuration snapshot |

### evaluation_results

| Column | Type | Description |
|--------|------|-------------|
| id | String | Primary key |
| evaluation_run_id | String | Foreign key to evaluation_runs |
| test_case_id | String | Reference to test case |
| question | Text | Test question |
| ground_truth | Text | Expected answer (optional) |
| retrieved_context | Text | Context retrieved from RAG |
| llm_answer | Text | Generated answer |
| judge_correct | Boolean | Judge verdict |
| judge_reasoning | Text | Judge explanation |
| judge_factual_errors | JSON | List of errors found |
| judge_missing_info | JSON | List of missing information |
| created_at | DateTime | When result was created |

## Troubleshooting

### Evaluation initialization fails

**Issue**: `INITIALIZE_EVALUATION=false but OPENAI_EVALUATION_API_KEY not set`

**Solution**: Set `OPENAI_EVALUATION_API_KEY` in `.env` file

---

**Issue**: `No PDF files found in evaluation_test_docs`

**Solution**: Add PDF documents to `evaluation_test_docs/` directory

---

### Evaluation run fails

**Issue**: `Failed to load test cases`

**Solution**: Ensure `evaluation_test_docs/test_cases.json` exists and is valid JSON

---

**Issue**: Judge service timeout

**Solution**: Increase `OPENAI_EVALUATION_TIMEOUT` (default: 60s)

---

### No results returned

**Issue**: Evaluation collection not found

**Solution**: Check logs for evaluation collection initialization errors. Delete `data/vector_db` and restart to reinitialize.

## Disable Evaluation

To disable evaluation system:

```bash
INITIALIZE_EVALUATION=false
```

This will:
- Skip judge service initialization
- Skip evaluation repository setup
- Skip test collection initialization
- Evaluation API endpoints will return 500 errors

## Future Enhancements

- Ground-truth based evaluation (compare against expected answers)
- Test data generation endpoint (auto-generate questions from documents)
- Category-based metrics (accuracy per question category)
- Evaluation comparison (compare multiple runs)
- Background task execution (async evaluation for large test sets)
- Export results to CSV/JSON
- Web UI for viewing results
