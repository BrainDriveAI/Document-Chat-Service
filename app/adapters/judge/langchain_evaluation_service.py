import logging
from typing import Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from ...core.ports.judge_service import JudgeService
from ...core.domain.entities.judge import JudgeOutput
from ...core.domain.exceptions import JudgeServiceError

logger = logging.getLogger(__name__)


class JudgeEvaluation(BaseModel):
    """Pydantic model for structured judge output"""
    correct: bool = Field(description="Whether the answer is factually correct based on the context")
    reasoning: str = Field(description="Detailed reasoning for the evaluation")
    factual_errors: list[str] = Field(
        default_factory=list,
        description="List of factual errors found in the answer"
    )
    missing_information: list[str] = Field(
        default_factory=list,
        description="List of important information missing from the answer"
    )


class LangChainEvaluationService(JudgeService):
    """LangChain-based evaluation service using OpenAI for factual correctness evaluation"""

    def __init__(
        self,
        api_key: str,
        model_name: str = "gpt-5-mini",
        timeout: int = 60
    ):
        """
        Initialize the evaluation service.

        Args:
            api_key: OpenAI API key
            model_name: OpenAI model to use for evaluation
            timeout: Request timeout in seconds
        """
        self.model_name = model_name
        self.timeout = timeout

        # Initialize ChatOpenAI with structured output
        self.llm = ChatOpenAI(
            model=model_name,
            api_key=api_key,
            timeout=timeout,
            temperature=0.0  # Use temperature 0 for consistent evaluation
        ).with_structured_output(JudgeEvaluation)

        # Define the evaluation prompt
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert evaluator assessing the factual correctness of AI-generated answers.

Your task is to evaluate whether the LLM's answer is factually correct based ONLY on the retrieved context provided.

Evaluation criteria:
1. The answer must be supported by information in the retrieved context
2. The answer must not contain fabricated or hallucinated information
3. The answer should address the question appropriately
4. Missing information is acceptable if it's not in the context, but mark it as missing

Provide:
- correct: true if the answer is factually accurate based on context, false otherwise
- reasoning: detailed explanation of your evaluation
- factual_errors: list any factual errors or hallucinations found
- missing_information: list important information that should be included but is missing"""),
            ("human", """Question: {question}

Retrieved Context:
{retrieved_context}

LLM Answer:
{llm_answer}

Evaluate the factual correctness of the LLM answer based on the retrieved context.""")
        ])

        self.chain = self.prompt | self.llm

    async def evaluate_answer(
        self,
        question: str,
        retrieved_context: str,
        llm_answer: str,
        ground_truth: Optional[str] = None
    ) -> JudgeOutput:
        """
        Evaluate the factual correctness of an LLM answer.

        Args:
            question: The original user question
            retrieved_context: The context retrieved from the document store
            llm_answer: The answer generated by the LLM
            ground_truth: Optional ground truth answer (not used in current implementation)

        Returns:
            JudgeOutput with evaluation results

        Raises:
            JudgeServiceError: If evaluation fails
        """
        try:
            logger.info(f"Evaluating answer for question: {question[:100]}...")

            # Invoke the LangChain chain
            result: JudgeEvaluation = await self.chain.ainvoke({
                "question": question,
                "retrieved_context": retrieved_context,
                "llm_answer": llm_answer
            })

            # Convert to domain entity
            judge_output = JudgeOutput(
                correct=result.correct,
                reasoning=result.reasoning,
                factual_errors=result.factual_errors,
                missing_information=result.missing_information
            )

            logger.info(f"Evaluation complete: {'CORRECT' if result.correct else 'INCORRECT'}")
            return judge_output

        except Exception as e:
            logger.error(f"Judge service evaluation failed: {str(e)}")
            raise JudgeServiceError(f"Failed to evaluate answer: {str(e)}")
